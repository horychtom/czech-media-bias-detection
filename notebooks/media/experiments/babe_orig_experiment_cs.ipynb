{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4db831f2-fa21-428c-87fe-983f42a5269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_metric,load_dataset,Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding,RobertaForSequenceClassification,AdamW,get_scheduler,TrainingArguments,Trainer\n",
    "from corpy.morphodita import Tokenizer\n",
    "from newspaper import Article\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import csv\n",
    "import gc\n",
    "import re\n",
    "\n",
    "model_checkpoint = 'ufal/robeczech-base'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "transformers.logging.set_verbosity(transformers.logging.ERROR)\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee713e6d-8dce-49b4-ba44-3b5fdd62579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def compute_metrics(testing_dataloader):\n",
    "    metric = load_metric(\"f1\")\n",
    "    metric2 = load_metric(\"accuracy\")\n",
    "\n",
    "    model.eval()\n",
    "    for batch in testing_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric2.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "        \n",
    "    return (metric.compute(average='micro'),metric2.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3d363-87e8-4793-b753-2b516be15bde",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f8bb581-daf7-4a7b-9307-fd2ca1ec7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/BABE/final_labels_SG2.csv',sep=';')\n",
    "data = data[['text','label_bias']]\n",
    "final_indices = data.index[data['label_bias'] != 'No agreement'].tolist()\n",
    "data = data[data['label_bias']!='No agreement']\n",
    "\n",
    "mapping = {'Non-biased':0, 'Biased':1}\n",
    "data.replace({'label_bias':mapping},inplace=True)\n",
    "data_en = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1900c9fd-e621-4de4-bd6c-8d7cf0544f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/BABE/texts_CS.txt','r') as f:\n",
    "    sentences = [sentence.strip('\\n') for sentence in f.readlines()]\n",
    "    sentences = list(filter(lambda x: len(x) != 0, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02a1fe92-b1d9-4659-8f5d-965610366066",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sentences)[final_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a93b05a0-3df7-41f0-accd-dd366699749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_dict({'sentence':sentences,'label':data['label_bias']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23619da5-0547-460a-a81e-c79431289cc4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21e99242-8bec-4015-bad1-5b925931277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a55195a5-5837-4dff-a84d-dfe2afcfc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False) #fast tokenizer is buggy in RoBERTa models\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64cb3511-6300-46bf-af7c-5be598f3813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda data : tokenizer(data['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d815a4bb-ae2f-4ab7-ac31-20d63790fb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dab9c74f205449babf608713ecb296f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(tokenize,batched=True)\n",
    "tokenized_data = tokenized_data.remove_columns(['sentence'])\n",
    "tokenized_data.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7e2fd6d-7277-4487-a172-2f4f753365fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=BATCH_SIZE,  \n",
    "    logging_steps=25,\n",
    "    disable_tqdm = False,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b75f54-b906-44cb-9c52-ff713ff1d9bf",
   "metadata": {},
   "source": [
    "### 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "11d5d4e1-47e7-4431-89a9-ed37645b51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d398fe99-daa1-49fa-acca-ecf7aa3f24db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/torch_formatter.py:44: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:135.)\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "***** Running training *****\n",
      "  Num examples = 2938\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.039300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2938\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.237900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2938\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.424500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.210900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.049100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2939\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.305200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.032800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "/home/horyctom/venv/lib/python3.8/site-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(array, copy=False, **self.np_array_kwargs)\n",
      "loading configuration file https://huggingface.co/ufal/robeczech-base/resolve/main/config.json from cache at /home/horyctom/.cache/huggingface/transformers/967e55aeea0667ffcda38959128e06f755d387fa034ffb448cab0851f27c5104.ae62083e57028e6866dba352dfd4261396c2f0e8978f299e3a17c055c564de09\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 51961\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ufal/robeczech-base/resolve/main/pytorch_model.bin from cache at /home/horyctom/.cache/huggingface/transformers/a08b09622106c5b39eba8d15008b404679d71abc2258f2f4c1ecb50492c19db3.9845f6e21d70dd41c9b0343f0fe86ace188d94128a6cf9689a5e01a85794a850\n",
      "Some weights of the model checkpoint at ufal/robeczech-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ufal/robeczech-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 2939\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 02:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in skfold.split(tokenized_data['input_ids'],tokenized_data['label']):\n",
    "    \n",
    "    token_train = Dataset.from_dict(tokenized_data[train_index])\n",
    "    token_valid = Dataset.from_dict(tokenized_data[val_index])\n",
    "    \n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_checkpoint);\n",
    "    trainer = Trainer(model,training_args,train_dataset=token_train,data_collator=data_collator,\n",
    "                      tokenizer=tokenizer)\n",
    "    trainer.train()\n",
    "    \n",
    "    #evaluation\n",
    "    eval_dataloader = DataLoader(token_valid, batch_size=BATCH_SIZE, collate_fn=data_collator)\n",
    "    scores.append(compute_metrics(eval_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "220ab629-9d51-46ee-b8f7-a96dcc5c9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),'../cs_babe.pth')\n",
    "model.load_state_dict(torch.load('../cs_babe.pth'))\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b73aea12-d63a-4db8-b051-8c174d450e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'f1': 0.782312925170068}, {'accuracy': 0.782312925170068}),\n",
       " ({'f1': 0.7945578231292517}, {'accuracy': 0.7945578231292517}),\n",
       " ({'f1': 0.7904761904761904}, {'accuracy': 0.7904761904761904}),\n",
       " ({'f1': 0.7656675749318801}, {'accuracy': 0.7656675749318801}),\n",
       " ({'f1': 0.7697547683923706}, {'accuracy': 0.7697547683923706})]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc734a9-1926-4a92-b89d-f510f41b8d1d",
   "metadata": {},
   "source": [
    "## Training on full data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b5279-f1c0-4900-bdbf-21869293b16a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3673\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='766' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 766/1150 02:27 < 01:13, 5.20 it/s, Epoch 6.65/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.445900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.441100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.331100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.078200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../checkpoint-500\n",
      "Configuration saved in ../checkpoint-500/config.json\n",
      "Model weights saved in ../checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ../checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ../checkpoint-500/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint);\n",
    "trainer = Trainer(model,training_args,train_dataset=tokenized_data,data_collator=data_collator,\n",
    "                      tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005f7924-fc91-4822-8d7b-e1ab62139f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'../cs_babe.pth')\n",
    "model.load_state_dict(torch.load('../cs_babe.pth'))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43ec87-2cd2-4af8-bf0c-5d8527fdcef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b17a7d8-b8e7-45fc-aa8e-011d5c805e72",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inferrence experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea92e7f9-a0c0-41e1-bfcd-4398bcff1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(sent:str):\n",
    "    toksentence = tokenizer(sent,truncation=True,return_tensors=\"pt\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        toksentence.to(device)\n",
    "        output = model(**toksentence)\n",
    "    \n",
    "    classification = F.softmax(output.logits,dim=1).argmax(dim=1)\n",
    "    \n",
    "    return {0:'unbiased',1:'biased'}[classification[0].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f50d87-9d93-4cab-94df-2c873f7b26ce",
   "metadata": {},
   "source": [
    "### Nice pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c58106b9-13ca-4e0b-a60b-7cc81f313898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbiased\n",
      "biased\n"
     ]
    }
   ],
   "source": [
    "print(classify_sentence(\"Od poslednho tdrho veera a nsledujc noci na m mma ani jednou nepromluvila.\"))\n",
    "print(classify_sentence(\"Od poslednho tdrho veera a nsledujc noci na m guru mma ani jednou nepromluvila.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d632fcdc-d255-442a-9b59-d5173c7b4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unbiased\n",
      "biased\n"
     ]
    }
   ],
   "source": [
    "print(classify_sentence('Podle rnsk vldy bylo sestelen IR 655 \"Vincennes\" mysln provedenm a nezkonnm inem.'))\n",
    "print(classify_sentence('Sestelen IR 655 \"Vincennes\" bylo mysln provedenm a nezkonnm inem.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665100b-40e7-4a58-8e3d-30e4972e017b",
   "metadata": {},
   "source": [
    "### CW-hard biased data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d5fd9b8d-d323-46d2-8202-51e21ceaed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"biased_cwhard_cs.txt\",\"r\") as f:\n",
    "    cw_hard_cs = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0adf11f4-afcf-4bb9-9170-c078900ae453",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = np.array(list(zip(cw_hard_cs,list(map(classify_sentence,cw_hard_cs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6cb719bd-3c93-40a9-bf71-de0bf9059ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias level:  32.44709712425393 %\n"
     ]
    }
   ],
   "source": [
    "stats = np.unique(annotations[:,1],return_counts=True)\n",
    "print(\"bias level: \",stats[1][0]/stats[1].sum()*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33c1a4-b355-4f3a-894c-653ac3bf4fe8",
   "metadata": {},
   "source": [
    "### Try on any article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e4aadf2c-46d8-48e3-9ce2-cb7d280fe1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias level:  52.32974910394266 %\n"
     ]
    }
   ],
   "source": [
    "article = Article('https://nazory.aktualne.cz/komentare/jak-jsem-na-stedrej-den-zradila-nasi-antivax-familiji/r~93a757c460d211eca1070cc47ab5f122/')\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "tokenizer_morphodita = Tokenizer(\"czech\")\n",
    "\n",
    "all = []\n",
    "for sentence in tokenizer_morphodita.tokenize(article.text, sents=True):\n",
    "    all.append(sentence)\n",
    "    \n",
    "sentences = np.array([' '.join(x) for x in all])\n",
    "annotations = np.array(list(zip(sentences,list(map(classify_sentence,sentences)))))\n",
    "stats = np.unique(annotations[:,1],return_counts=True)\n",
    "\n",
    "print(\"bias level: \",stats[1][0]/stats[1].sum()*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "45aeee91-89f1-4b1f-93a1-476957131642",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Od poslednho tdrho veera a nsledujc noci na m guru mma ani jednou nepromluvila .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['\" Zrdce nroda , \" vykikla mma a bouchla dlan do volantu , \" zrdce zasranej \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jo , abych nezapomnla , ono to sem pat , zrdcem nroda je njakej epidemiolog , jmno mi uteklo , kterej nabd , a se lidi okujou .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['On je typickej podpantoflk , ale posledn dobou se taky maliko zmnil a obas aspo ppne , asi u je toho i na nj trochu moc .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jemu neodpovdla , ale otoila se na m dozadu , pitom jela na zk silnici pes osmdest , a povd : \" Chpej , Mono , \" jemarj , jak j nenvidm to sv jmno , Monika , ale k plnmu lenstv m dohn Mona , Monik a Moniek , vradila bych kvli tomu , \" nekej mi Mono , prosm , \" zapla jsem a ona sadisticky , \" chpej , Moniku , \" dodvm , e mi je dvaadvacet , ne tyi , \" j do t zatky vi - d - la . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Hele , j samozejm vdla , e jsem s nima nemla jezdit , jasn e jo , vdla jsem , e to bude peklo , navc tam pijede sgra , co je moje mma na druhou , take peklo na druhou , jenome se jet nikdy nestalo , e bych s celou na rodinou netrvila tdrej den .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No a mn je blb jim to kazit , oni by mou neptomnost , jak ekla mma , kdy jsem se z toho letos snaila vymyknout , povaovali za \" znamen , e s nmi nechce nic mt \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Hele , upmn , thle generaci fakt nic nevysvtl , je to marn .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Souvisejc Takov obyejn covidov tdr veer sbhnuv se v Podkrkono',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mlela jsem , protoe to byl jedinej zpsob , jak mmu donutit , aby aspo chvli koukala na silnici jinm ne duchovnm zrakem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Tta teda dv pozor , kdy ona kouk dozadu , jenome on je naprosto zoufalej ofr , take kdyby shl na volant , to si pite , e by ns strhnul rovnou do protijedoucho autobusu .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Chpejte , tahle famlije miluje , kdy se mno , milujou , kdy jich je m dl vc , njak pelidnn jim vbec nic nepovd , take kdy jsem ekla , e s sebou vezmu svho kluka , chodme spolu u ti msce a jeho rodie u od jeho osmncti ijou v Jin Americe , tak naden kejvla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Na to jsem zalhala , e nevm , i kdy samozejm vm , e je okovanej , ale kdy u s nima jedu , chci ho mt s sebou , abych to peila , a chci ho uetit jejch ujetejch ezoterickejch pednek , opravdu nerada bych o nj pila , dneska najt kluka je fakt porod .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nejsem si s nm sice pln \" sure \" , to ne , ale to neznamen , e budu dal msce sama .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No a tady jsme u toho , radi to vysypu hned , protoe to je te , jak k mma , \" asi to pln nejdleitj \" , toti okovn proti covidu a jej boj proti \" tomu neskutenmu svinstvu \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jenome se j to nepovedlo , nebo jim to nefungovalo , nebo co , nevm , pak to zkusila dal den a dal a nic , dneska si myslm , e nco musela dlat blb , ona nen zrovna ajk , pod se j to nedailo , tak jsem nabdla , e j pomu , ale ona klasicky \" nejsem snad jet plnej blbec , nedlej ze m tak stran neschopnou enskou , vy mlad si myslte , e umte vecko nejlp \" , a zase se j to nepovedlo .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Sedla u ns v kuchyni a kiela na svtc tablet \" j ten krm nenvidm \" a buila pst do stolu , a mi vylila kafe .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Andlsk kyvadlo , promluv !', 'biased'], dtype='<U588'),\n",
       " array(['Dal den jsem ji vidla v kuchyni , jak sed s tm svm milovanm andlskm kyvadlem - ona u je toti dvno \" ezo \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Soustedila se na ten etzek s vybrouenm prhlednm jehlanem na konci pln propadl do sebe , koukala , jak se kejv , nebo nekejv , nebo co , o mn nevdla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['ekla mi , e se \" jenom dotazuje \" .', 'biased'], dtype='<U588'),\n",
       " array(['Povdm , na co a eho nebo koho se dotazuje , nae ekla neptomnm hlasem , \" tm \" hlasem , rozumj ezo - hlasem , takovm cizm , vzdlenm , a ji laskav nerum , tak jsem sedla a koukala na ni , dlouho to trvalo , a najednou vykikla \" tak ne ! \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['A hned vytoila slo sgry a hustila do n , jak j andlsk kyvadlo eklo , e ne , ale ne e ne jen ona , ale nikdo z ns ne , cel rodina ne , a e je to \" otzka ivota a smrti \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['S andlskm kyvadlem blbne u dlouho , s ttou to vbec nemme rdi , ale j se fakt tko odporuje , ona je vd , autoritativn typ .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Kdy jsem se j , mimochodem ironicky , zeptala , jak to me fungovat , najednou mluvila o dost jinm hlasem , jako by ani nevychzel z n , tak jako tie a chraplav a spav , a vysvtlila mi - jasn e nevysvtlila - jak se \" tzajc mus pln oprostit od svho ega , \" v thle poloze hned pejde do spisovn mluvy , \" a hlavn , hlavn od veker racionality , pot mus zat sledovat jemn , takka nepostehnuteln , mimovoln svalov pohyby , je vychzej z nervov soustavy , je v tu chvli na sebe bere roli vodie univerzln energie a umon ony mimovoln pohyby sval .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['\" E - ner - gi - i , \" ekla drazn , protoe j jako zasvcen je zejm jasn , co je to za energii , zatmco nezasvcenejm mimom mho typu to nem smysl vykldat .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Dost m zarazilo , jak najednou mluv o prci s kyvadlem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Proboha , jak prce ?', 'biased'], dtype='<U588'),\n",
       " array(['\" To by nemlo smysl , \" ekla mi tehdy pke . - Souhlasm .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No a od t doby , co se nedokzala zaregistrovat na prvn dvku , se promnila v totln antivaxerku .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Sgra to sam , jen jet umocnnj , Hedvika je toti navc pekeln militantn a kadej , kdo se okuje , je podle n idiot a zrdce nroda - to m mma od n - a \" otrok bez vlastn vle \" a ubok a \" lovk bez vlastnho j \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mn sgra ekla asi stokrt , e jsem \" jen otrok bez vlastn vle \" , nikdy m nemla rda , nevm pro , a mm pocit , e covid j k tomu dodal njakou novou , mma promine , e - ner - gi - i .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Abyste si nemysleli , nejsem dnej rodinnej odboj , stejn jako tta nen dnej antimmovskej partyzn , to ne .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Take zrazujeme jenom sami sebe .', 'biased'], dtype='<U588'),\n",
       " array(['Snad nemusm ani dodvat , e mma i sgra jsou samozejm taky proti roukm a respirtorm , vude vykldaj , jak jsou nebezpen , a ob chodj na antivaxersk demoky .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Take jsem j Pu musela pedstavit a ona na nj hned zaala chrlit , v jak stran dob , v jak stran nesvobod , totalit ijeme .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Vidla jsem , jak j nerozum , sypala to na nj jak trk z nklaku a on je takovej tichej , to se mi na nm lb , nic nikomu necpe , a pitom je chytrej , v pln vechno , ale dr se stranou , a kdy jsme pak spolu , sami dva , tak k neuviteln vci , pln se stydm , jak j nic nevm , kdeto on v fakt vecko .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mma s nm stla v pedsni a kzala mu , bhem pr msc se z n stala fakt kazatelka , jak \" nm oni jen ni imunitu , protoe imunita , Pirozen Imunita \" , ona to opravdu vyslovuje jako Pirozen Imunita , s jakousi ctou a pokorou , vdycky si toho vimnu a vdycky m to dost vyds , m dl vc m ta jej Pirozen Imunita ds , \" tedy to podstatn , to , o tady jde , je nam kolem zde na Zemi \" , jo , Zemi taky vyslovuje s velkm Z , \" mme pstovat imunitu , otuovat se , cviit , ale hlavn , jak se jmenuje , Mono ? \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Stla jsem tam s nimi a modlila se , aby si Pa nemyslel , e jsem stejnej magor jako mma , ale on , jak je fakt chytrej , tak prost jen stl a mlel a drel , neusmval se , tvil se smrteln vn a to se mm lbilo .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak mi ekla , e \" to bude njakej vnmavej kluk , toho se dr , ten by se mohl nauit i s andlskm kyvadlem , kdy ty to jako agnostik odmt \" . - Agnostik ?',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Co blbne ?', 'biased'], dtype='<U588'),\n",
       " array(['Kouk se na m a povd \" nic \" .', 'biased'], dtype='<U588'),\n",
       " array(['\" Jak nic ?', 'biased'], dtype='<U588'),\n",
       " array(['To kzn v pedsni s tebou nic neudlalo ? \"', 'biased'],\n",
       "       dtype='<U588'),\n",
       " array(['No a te sed vedle m v aut a nemu nevidt , jak m pi t len jzd zaat vechny svaly , protoe se boj .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Po revoluci mu ho vrtili , mma statek udruje , star se o nj , cpe do nj neskuten prachy a dala ho cel rodin k dispozici .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Abyste to sprvn chpali , mma je hodn , obtav , tdr , to jen ten covid j uvrhnul do takov divn , kazatelsk , nenvistn polohy .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Vichni u byli na statku naskovan a vichni samozejm nenaokovan a bez respirtor , protoe od lta mma vechny fakt dokonale zblbla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Ona prost je siln , pesvdiv osobnost , notabene oni ti , oba strejdov a mma , to asi maj njak geneticky dan , dda byl pesn tenhleten typ \" vdycky proti vemu a proti vem \" , samozejm proti komunistm , to po nm ale mmin brati nepoddili , a zrove by se rozdal , a vecko pro rodinu .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['J to mla hor , cel ta parta jsou antivaxei a antiroukai a vyznavai mmin Pirozen Imunity , co dvaj najevo opravdu brutln lbakou , dv to zdaleka tolik neprovozovali .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Prost se na lovka vrhnou , mm pi tom neodbytnej pocit , e jim covid sed pmo v mozkovm centru a naizuje : te na ni sko a dn na tv , lbej ji na rty .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Dovedete si asi pedstavit , jak je to nechutn .', 'biased'],\n",
       "       dtype='<U588'),\n",
       " array(['Jeden imunolog toti nkde na netu kal , e funguje pravideln vyplachovn a kloktn , konkrtn zmnil vincentku do nosu a listerinem \" prokloktvat \" - to slovo je bo - krk .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Musk st rodiny vyjma mho tty a Pti , kterej vypadal po celou dobu pobytu dost vyden a nebyla jsem si jist , jestli se mnou zstane i po tomhle anti tdrm dnu , se intenzivn ethylizovala slivovic od strejdy  . 2 , teda mladho strejdy .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Manelka strejdy  . 1 , tedy starho strejdy , poznamenala , e povauje za velmi rozumn dt nejdv dti spt , a chce jen upozornit , e Honzk - tletej , nesnesiteln rozmazlenej harant , kterej si vecko vynucuje intenzivnm jeenm a evem a rodie mu to tolerujou , protoe \" dt se mus nechat projevit a nesm se omezovat \" - m prej \" u dva dny bolesti bka a stran , ale opravdu stran prjem , dokonce chudek i zvracel \" , take k nmu asi bude muset obas odbhnout a \" doufm , e vs Honzk nenakaz \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mma ekla tm svm nov objevenm , knskm tnem a jaksi za ns za vechny : \" Hanko , neboj se , my mme pece Pirozenou Imunitu . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No , nezdlo se mi , e by se vem dky t jej slavn P . I . njak ulevilo , a zaslechla jsem , jak strejda  . 2 k tet  . 2 \" taky to mohla ct dv \" , za co ho moje mma , kter nic neujde , sjela velmi zlm pohledem , jak vrh guru se svatoz na nevcho psa .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pi obd mi bylo lto tty , on jako by tam snad ani nepatil , jako by ho tam byla zapomnla njak jin rodina .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Po obd jsem la za nm a povdm \" tati , dobr ? \" a on se jen usml a pokril rameny a velmi potichu ekl \" hele , j vm a ty v , nepatm sem \" , co je svat pravda .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Usml se tak vdn , e mi ho bylo lto jet mnohem vc .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak jsme li s Pou do mho ajnclku .', 'biased'], dtype='<U588'),\n",
       " array(['Kdy jsem na statku , tak tam mm vyhrazenou takovou nudli komrku na kraji seikmen stechy , ned se tam sice stt , jen leet , ale mm tam dlouhou a dost irokou postel , kam jsme si spolu vlezli a milovali se , co Povi jde , asi e u toho me bejt oficiln zticha .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Kdysi mma poutla televizi , pohdky a koledy , ale postupn se to mnilo a posledn lta byla vdycky celorodinn kontemplace , pi n lo pedevm o to , aby rodie dokzali zpacifikovat ty sv neskuten ukouran dti .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['A loni , loni se mma pokouela vtit budoucnost cel famlije pomoc andlskho kyvadla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mimochodem , ne e bych t ndobce ve tvaru jehlanu z vybrouenho kamene zaven na stbrnm etzku vila , ale to , e pandemie covidu neskon , a to , e ns ek \" velk zatkvac zkouka \" , to teda kyvadlo trefilo celkem pesn .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Ped vee jsme si vichni vetn harant museli povinn poslechnout toho herce Duka , co chod bosej , kolem hlavy m auru jak svatej na most , prs se len nezlomnou imunitou a v na sebe , nikoli na vdu a okovn .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pa to poslouchal , pak se na m kouk , a j vidla , co mu rotuje v t jeho chytr , mlenliv , feck hlav , \" proboha , kam jsem to zapad \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nkolik tejdn po thle na zatkvac zkouce mi zniehonic ekl \" j tam ten veer fakt myslel , e snad dojde i na njakou dtskou ob \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Vypla jsem ve chvli , kdy Duek ekl \" jsem takovej lovk , kterej od roku 1991 neml v tle lk , j nechodm vlastn k doktorm , dlm psty , plavu ve studen vod , v ece , cvim dechov cvien \" , protoe j chodm odmalika k doktorm a nebejt lk , u tu nejsem , ale to sem nepat .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak se jedlo , a ta tdroveern tabule byla taky o hodn jin ne jindy , mlem bych ekla , e covid vecko zmnil , ale blbost , to my jsme vecko zmnili .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jenome nkdo u kapra nechce , tak se servroval losos , fil , zky a kuec stehna jak nkde v restauraci s dlouhm jdelnkem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak ly tety dt spt dti a pijela Hedvika , moje sgra , s tm tpkem , co s nm ije , ale nevzali se .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['On je hodn zazobanej , m novho bavorka , \" BMW je auto \" je jeho oblben a hojn opakovan hlka , jako e nic jinho auto nen , a nsledujc hlka vdycky zn \" Tesla nen auto \" , protoe elektroauta tenhle mudrc povauje za \" nejvt omyl v technick historii lidstva \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mv sice tmahle heslkama , ale nae Hedvika ho d jak sktr , o tom dn .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Take je samozejm antivaxer a chod s nima na demoky , piem jeho osobnm pnosem je , e se vdycky zabal do esk vlajky .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Kdy tam s nm za mnou Hedvika nedvno pila , Even si s tou vlajkou chvli hrl - jo , on se fakt jmenuje Even , pekeln jmno - , a pak povd \" blbej poet hvzd , Mono , sorry jako \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['J jen abyste mli pedstavu , co je Bavork za tpka .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Sgra se na mho kluka hned vrhla , skoila na nj jak tygr na antilopu , vyplila mu pusu na rty , a nekecm , fakt to vypadalo , jako kdyby se do nj zakousla , a jeela \" j , tak ty jse ten P -  , j ? \" pitom j jsem j o nm nic neekla , take asi mma .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pa ekl \" ano , j jsem Petr \" a j se jen divila , e mu ze rt nestk erven , mon to jako medik um njak zastavit .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Strejda  . 2 vechny obhl , on mmu odjakiva poslouch na slovo , co tetu  . 2 pomrn dost nadzvedv , ale nikdy mm nic neekne , a jdou dol .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Tlet bolav bko njakm zzrakem usnulo , bu mu mma poslala kus t sv svat energie , nebo mu dali cucat cumel s mkem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Podle vzoru m mmy prakticky vichni vzeli v tom nejtsnjm covidovm klini a mumlali nco jako \" hlavn Slu , Energii a Pirozenou Imunitu \" a dvali lbaku .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pa si drel distanc a huel nco jako \" pkn svtky \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mm dojem , e se mu podailo s nikm se neobjmat , co je , hdm , podobnej spch jako dt K dvojku bez kyslkovejch pstroj .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Po covidem pohrdajcm vnonm vinovn nsledovala spolen meditace .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Chlapi odnesli stoly , sedlo se na zemi , komu klouby dovolily , sedl v lotosku , mimochodem Pa d lotos pln bez problm , on je fakt mimodnej , komu klouby nedovolily , tak se tam njak nepohodln , ale poslun kril .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Guru mma pustila uhozen meditativn bzuen - huen - mruen , pivezla si na to nov velk bluetoothov dbelko , kter se j stopro pokusm bhem ledna nenpadn zabavit .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Meditace trvala pesn 27 minut a konila tak , e hudba la do ztracena a nae knka zaala hrozn divn huet zavenou pusou a nosem a postupn se pidvali vichni krom Pti , m a tty , ale tta tam vlastn nebyl .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nkam se vypail .', 'biased'], dtype='<U588'),\n",
       " array(['Jak jsem tak koukala na pbuzn okolo sebe , zdli se mi njak bled a z formy , divn , ale ono se taky pi meditaci svtilo jen svkama , tak to asi zkreslovalo .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Mla takov divn oi , jako by plovouc , podobn vypad jeden mj kolega z filozofick fakulty , kterej droguje , taky tak jako vdycky plave , a povd mu : \" Dokzal jsi vnmat tu spolenou Energii a Slu ? \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pa se kouk na m , pak na ni , pak do zem a neek nic .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Zdlo se mi , e Pa poprv oil , a byla jsem v oku , kolik je schopen toho sladkho hnusu s pomeranovou vou do sebe vylejt , a taky e byl u zhruba za pl hodiny docela fest vylitej a obliej mu zervenal tak , jak jsem to u nj za cel ty ti msce naeho chozen nevidla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jak se vylil , ml se ke mn mnohem vc , co , musm ct , mi nebylo nepjemn , protoe v cel t na rodin se vlastn nikdo k nikomu nijak nem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Prochzme se po havm', 'biased'], dtype='<U588'),\n",
       " array(['Vichni jsme si museli stoupnout okolo toho ttova improvizovanho havit , nkter u docela slun vrvorali , Pa v tom pmo exceloval , a mma drela tdroveern promluvu , jakousi modlitbu k Pirozen Imunit nebo jak to nazvat .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Zakonila ji slovy \" a nyn vs zvu , vyzujte si stevce \" , nikdy jsem ji neslyela pout slovo stevce , jako kdybychom se ocitli o sto let zptky , \" a projdte se po havm uhl na dkaz va osobn , neimplantovan odolnosti \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nejdv jsme se ale museli spolen rozdchat , pak jsme toili rukama okolo hlavy a zase jsme mli huet , stejn jako huela mma zavenou pusou a nosem .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nikdo ani neppl , ale mj tta , nikdy bych to nj neekla , do ticha nahlas pravil , e \" samozejm nikdo nen nucen \" , nebo \" my , oproti sttu , nikoho k niemu nenutme a chze po havm uhl je zcela dobrovoln \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Vidla jsem , jak se mm nelb , e si dovolil nco ct .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pak se stalo jet nco , co urit nebylo pedem dohodnut , tta si toti sm zul svoje ern , slavnostn polobotky , kter nos vhradn do divadla , sthl si i sv ed ponoky , piem bylo vidt , e m na obou palcch a na lev pat obrovsk uzny , napadlo m , e pesn proto se zul , e sleduju jeho jakousi tichou pomstu na knce za vechnu tu msce i sp roky trvajc manipulaci , el do ela k mm , ohrnul si nohavice , a ani by se j zeptal , piem bylo jasn , e prvn chtla lapat po havm prv ona , vyrazil .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nijak nespchal , prost to sv havit peel a m napadlo , e to je cel mon jen njakej fake , e to nejsou prav hav uhlky , dokonce jsem se naklonila a dala nad n ruku , ale fakt to tam fest slalo a j byla snad poprv v ivot na ttu pyn .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No , co si myslte , kdy jsme se o dva dny pozdjc vraceli autem do Prahy a mma se ns zase pokouela narvat pod asi deset tirk , najednou ttovi ekla \" nemusel jsi m tedy pede vemi ponit \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Msto pitakn se mma pi naprosto nesmyslnm , sebe - vraednm a ns - vraednm pedjdcm manvru tila pmo na protijedouc VW transportr , pidvala plyn a v posledn chvli vz strhla tak , e nebt Sly Na Jej Stran , fakt to nemu jinak vyjdit , bylo po ns i po tch Nmcch , co freli proti nm v dobr ve , e nedj v blzinci .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Kdy tta peel loe ze havch uhlk , rozhldla jsem se a vidla , e si u tch lid okolo , u t ciz rodiny , kam se pienil jako manel  . 2 po zemelm , jako nhraka , a nepinesl dnej statek a nezplodil s mmou dn dal potomky a nikdy nikoho nim neoslnil , e si u nich poprv v ivot - a zejm taky naposledy - vyslouil kdy ne ctu , tak aspo obdiv .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Neekali za prv , e to pejde , a za druh , e si dovol pedbhnout rodinnou knku Imunity .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['J to najednou pochopila , uvdomila jsem si , co asi tta dl , bleskov jsem se zula , sthla jsem si punoche a vrazila na uhl krtce po nm . Mma se tvila jak obloha dv vteiny ped krupobitm .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Po mn , a fakt jsem k nmu poprv po tch naich tech mscch chozen cejtila nco jako lsku , el bosej Pa .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jet o nco pomalejc ne tta a j , pln v klidu , frajer .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['A pr tejdn po tom vypeenm tdrm dnu v Keplech mi vysvtlil , pro se tak klidn promenoval po havm uhl .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Dal moment , lidsk ke je taky dost patnej vodi tepla .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Pokud teda njakej zlomyslnk do uhlk nehod teba ztku od piva .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Neskuten chytrej tpek , no ne ?', 'biased'], dtype='<U588'),\n",
       " array(['Pitom , jak asi chpete , cel aranm bylo , ani to ovem komukoliv ekla , e pjde prvn , jako el prvn Moj skrz Rud moe .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Moc nevm , kdo potom pes uhlky jet el a kdo si netrouf , sthla jsem se s Pou a ttou do jdelny , vytvoili jsme takov divn , tajn okovan spoleenstv , ostatn zstali s guru mmou ve stodole , a tta se m najednou zeptal \" Mony , \" on v , jak nesnm Monu a Monika , i kdy jsem mu to nikdy neekla , prost to poznal , \" Mony , Petr to v ? \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nevdla jsem hned , co jako jestli Pa v , ale tta neekal , a mi to docvakne , a ek Povi \" aby sis nemyslel , e je to tu samej cvok , j jsem okovanej , jen to tu nikdo nev \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No , nebyla jsem si pln sure , jestli ta vta sp neznamen , e to tu naopak je samej blzen vetn ns dvou , ale jednak jsme byli u dost vylit , on ten pun fakt pe , a jednak se mi lbilo , jak mezi nma najednou vzniklo takov pln jin pouto , pouto tajn antisekty uprosted sekty .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jestli fakt o nco nestojm , tak se svlkat ped strejdou  . 1 a 2 a ped jejich synama a dcerama , fakt dky , ale ne .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Copak nejsi rda , e mi nic nen ?', 'biased'], dtype='<U588'),\n",
       " array(['Okolo plnoci zanali nkte lenov na slavn famlije vypadat hodn divn .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Ze svho novho dbelka a mobilu zaala poutt fakt devn muziku , hudbu Fra Fra , pokud to znte , Funeral Songs , to je asi kadmu jasn , Naked , jako e na tenhle svt pijde a z tohodle svta odejde s nim , Destiny a dal pecky .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Teda ne e by zpvali anglicky , j si to pak doma vecko vygooglila .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Fra Fra zpvali njakm africkm domorodm jazykem a psobilo to neuviteln pvodn , jako bysme se fakt penesli o ti tisce let dozadu .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Na m i na Pu , jak jsme byli velmi intenzivn zpunovan , vylit jak dbny , to fungovalo pln len , trsali jsme oba v transu , okolo ns trsalo pr dalch , ale mlo , mlo lid , taky mma , jene j vidla , e se s n nco dje .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Najednou mn j bylo lto , trsali jsme s Pou k n a j se ptm \" mami , je ti nco \" a ona tak divn \" je mi , je mi \" , co jsem nechpala .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['My s Pou tanili , j nevm , snad dv hodiny , furt na Fra Fra , furt dokola , ale j si vimla , e okolo ns u nikdo , jen mma , tanilo se v jdeln , stoly srazili do kouta .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Najednou mma klesla na kolena a zaala dvit , beli jsme j pomoct , mj medik na ni mluvil , jene ona e mus na zchod , je mi to blb popisovat , muselo to bejt fakt dsn , protoe to evidentn neudrela , jestli chpete , no a my ji thli k wc , mme na statku dva zchody , jene wc byly obsazen a ped nima stli ekajc v kech a zevnit byl slyet nek , tak jsme mmu thli ven , visela na ns jak hadr , jenome na dvoe dal lidi a vichni ve depu a vichni vzdychali a vude bylo vidt , e to z nich lt ven .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Rozpoutalo se prjmov peklo , vichni , pln vichni to chytli , jen j , Pa a tta nic .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nechpu , jestli to bylo tm tajnm okovnm , nevm , ale nae rodina tam s prominutm srala celej dal den , probhala permanentn rvaka o zchody a o dva plechov kbly .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Jak vypadal n dvr , to se mi ani nechce popisovat , stran , Pa s mm ttou to odveli smchan se snhem v kolekch ven za statek , tam , co je to velk hnojit .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Uvnit to vypadalo jak ve vlenm lazaretu , j vzala auto a jela do Suice shnt endiaron , ale kdy jsem se za dv hodiny vrtila s nkolika krabikama , tak na m mma jeela , e ona to laktobacily vradc svinstvo jst nebude , m imunitu , u to ale vyslovovala s malm i , a jej tlo si s tm \" mus poradit , mus \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Bylo j divn , e j tu dysentrii nebo co to bylo nemm .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['\" Jak je to mon , \" ptrala , \" zrovna ty , u kter bych teda dnou odolnost rozhodn neekala . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nkdy jsem fakt blb , a tak jsem si bohuel neodpustila poznmku \" asi mm lep imunitu \" a ona jak to prej myslm .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Zejm za to tak trochu mohl Pa , protoe mi ekl , e jednm z projev covidu me bejt i prjem , pitom bylo jasn , e to na Keply zavlekl ten malej , tletej mazel .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Chpete to naprosto sprvn , ttu jsem vynechala , tta se bl , e na nj bude mma guru natvan , take pedstral , jako e m taky prjem , dlal , e lt jako ostatn , dokonce dlal , jako e si pere spodky v koupeln , prost jel na jistotu a hrl ten tytr se vm vudy .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Kdy uslyela , co jsem ekla , zasyela \" take ty jsi zrdce \" a pak jet \" no jasn , pro m to vbec nepekvapuje \" .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Byl u toho i Even Bavork , ten jej zazobanej amstr , a najednou povd \" taky se nechm naokovat , dy tohle je hotov peklo a jim nic nen \" , za co okamit dostal ale neskutenej kart .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['No nic , na sge mi nesejde , stejn m nesn od minuty , kdy jsem se narodila , jenome Hedvika to samo sebou prskla mm .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['A bylo zle .', 'biased'], dtype='<U588'),\n",
       " array(['Mma mi ekla pouze toto : \" No to jsi mne , Moniko , straliv zklamala , to jsi zklamala celou nai rodinu , nic horho jsi mi asi nemohla provst . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['ekla jsem \" ale mami , copak nejsi rda , e mi nic nen , e nemm ten pekelnej prjem , e mi neselhala imunita ? \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Zeslblm hlasem na m kiela : \" Ty nemluv o imunit , ty sis to svinstvo nechala dt do tla , stran , stran jsi ns podrazila . \"',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Od toho tdrho veera a nsledujc noci na m mma guru nepromluvila .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Sice pod skoro nic nek , hodiny v klidu proml , ale je fakt milej .',\n",
       "        'biased'], dtype='<U588'),\n",
       " array(['Nebejt tty , kterho odmtm nechat s guru mmou samotnho , urit bych se k Povi okamit odsthovala , nabz mi to kadej den , co jsme se vrtili z Kepel .',\n",
       "        'biased'], dtype='<U588')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x : x[1] == 'biased',annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9137e02-24e8-45e1-a7f4-bfd99e9c190f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436089f-506a-43e6-be0c-619870db1cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
