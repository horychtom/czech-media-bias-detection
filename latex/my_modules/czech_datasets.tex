\chapter{Czech datasets}
Despite the relatively satisfying number of datasets, there is essentially no Czech dataset which is at 
\section{Translation}
\subsection{DeepL}
\section{Processing}
Since BERT can handle cased.
\section{Analysis}
output \= unified set of Czech bias related datasets
\textbf{Czech Unified set of Bias Data}
\begin{enumerate}
    \item mpqa-cs
    \item subj-cs
    \item newsb-cs
    \item cw-hard-cs
    \item wiki-npov-large-cs
    \item babe-cs
\end{enumerate}

\newpage

\section{Czech Wiki Neutrality Corpus}
Finally, I present two novel parallel corpora extracted directly from Czech Wikipedia. To my best knowledge, this is the only original Czech dataset related to media bias and subjectivity detection. The only related dataset is SubLex which is a subjectivity lexicon mainly focused on sentiment. However, lexicon based approaches proved to be insufficient in takcling complex media bias.

For the dataset creation, I followed two main existing approaches, both of them relying on the extraction of revisions that includes the \{\{NPOV\}\} tag or its variation. The NPOV tag has also its Czech version \Gls{nup}. However, the czech version is practically not used and so for the extraction, the english variations were used.

\subsubsection{WIKI1-CS}
For this dataset I followed the \cite{aleksandrova2019multilingual} approach and their script. First, a file with all pages and its complete edit history is downloaded from wiki dump. I used the "20220201" version. Then the edits containing one of the NPOV related tags are extracted and then the process of sentence extraction follows. All used tags can be seen in appendix.

This approach yielded 15k sentences, However, it uses rather trivial assumption that when NPOV tag is removed, all removed sentences are biased and all added are expected to be unbiased. This annotating strategy later [odkaz na experimenty] proved to be insuffiecient and yielded very noisy dataset. For this reason I excluded this dataset from further experiments.

\subsubsection{WIKI2-CS}
This dataset was created following \cite{pryzant2020automatically} approach. The process is the same as described in section \ref{wiki}. I used "20220201" snapshot of wikipedia dump. I chose the latest version that included all the necessary files.
I used the script publicly available on github [odkaz na repo],with few slight modifications to fit the czech language properties:
\begin{enumerate}
    \item Regex was extended to exclude czech words that contain "pov" inside eg. povstání, povlak etc. \footnote{Regular expression used to match npov related comments: }
    \item All cases has been preserved, since bert like models can handle cased language.
    \item Czech Morphodita tokenizer was used.
\end{enumerate}
\newpage
Final dataset consists of:
\begin{enumerate}
    \item 3k of "before" and "after" sentence pairs
    \item 1.7k subset of mentioned set where only one word was changed
    \item 7.5 sentences, where the change was rejected or reversed implying neutrality of the original sentence
\end{enumerate}

The random example of sentence pair can be seen in \ref{fig:wiki2cs-example}

\begin{figure}
  \includegraphics[width=\linewidth]{my_modules/multimedia/wiki2example.jpg}
  \caption{Example of CWNC sentence pair}
  \label{fig:wiki2cs-example}
\end{figure}

\section{Not translated}
Since my current DeepL plan allowed me to translate only one "large-scale" dataset due to the DeepL's fair usage policy I decided to not translate the NewsB since it focuses on distinguishing between conservative and liberal bias hence is not directly applicable on our task. However the hyperpartisan task is a good candidate for multi task setting as suggested later in \ref{mtl} section