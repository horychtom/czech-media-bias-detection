\chapter{Experiments}\label{experiments}
Finally, to complete the last research task proposed in the introduction \hyperref[problem_definition]{T4}, the datasets collected in section \ref{datasets} are leveraged to build a \textbf{media bias classifier}.

To do so, a transformer model (\ref{att_transformers}) is \textbf{fine-tuned} on the BABE dataset (\ref{babe}).


Furthermore, to answer the two research questions,\hyperref[Q1]{Q1} and \hyperref[Q2]{Q2} and to push the performance of the classifier, the effect of further pre-training\footnote{Primary pre-training is the original unsupervised pre-training task executed by the authors of the particular language model.} are studied.

A scheme of the whole process can be seen in figure \ref{fig:pipeline}. Firstly, a baseline fine-tuning is performed to select the suitable language model. Then a limited hyperparameter tuning of the model follows. Subsequently, the tuned model is then pre-trained on combinations of auxiliary datasets reviewed in section \ref{datasets}. The impact of the pre-training is studied through 1) direct evaluation on the BABE and 2) fine-tuning and evaluation on the BABE.

The optimal pre-training strategy and hyperparameters are then used to build the final clasifier.
 
\begin{figure}[h]
  \includegraphics[scale=0.5]{my_modules/multimedia/pipeline.png}
  \caption{Scheme of experiments.}
  \label{fig:pipeline}
\end{figure}
 
\section{Models}
Architecture-wise, a classifier consisting of a dense\footnote{Sometimes is referred to as fully connected layer.} layer is attached to the pre-trained language model to perform the binary classification task. The tested language models were pre-trained either solely on Czech data (monolingual) or on multiple languages jointly (multilingual). The scheme of the particular architecture used can be seen in figure \ref{fig:classifier}.

As opposed to English language, there is a relatively low number of Czech pre-trained language models available. A list and a brief summary of all the models tested can be found in the following:



\begin{figure}
\makebox[\textwidth][c]{
  \includegraphics[scale=0.3]{my_modules/multimedia/final_colored.png}
  \caption{Scheme of a text classification architecture used in the fine-tuning a BERT-based models.}
  \label{fig:classifier}
  }
\end{figure}



\begin{itemize}
    \item \textbf{RobeCzech} \cite{strakarobeczech} - RoBERTa-based model with 125M parameters. Like its original counterpart, it is trained with the \gls{mlm} task, on 4,917M tokens of Czech corpora.
    \item \textbf{Czert} \cite{sido-etal-2021-czert} - BERT-based model with 110M parameters, trained with \gls{nsp} tasks. All Together trained on 37GB of Czech text. 
    \item \textbf{FERNET-C5} \cite{lehevcka2021comparison} - BERT-based model trained with the \gls{mlm} and \gls{nsp} task on 93GB of Czech text from the Common Crawl project.
    \item \textbf{FERNET-News} \cite{lehevcka2021comparison} - RoBERTa-based model trained with \gls{mlm} task on 20GB of Czech News text.
    \item \textbf{SlavicBert} \cite{arkhipov2019tuning} - BERT-based model with 179M parameters, trained on four languages: Russian, Bulgarian, Czech, and Polish. The model is trained on all 4 languages at once. The model is not trained from scratch, but it is a fine-tuned version of mBERT.
    \item \textbf{mBERT} \cite{devlin2019bert} - BERT-based model with 179M parameters trained on corpora of 104 languages, including Czech, with MLM task.
\end{itemize}







\section{Experimental setup}
All models are fetched, trained, and evaluated using the HuggingFace API\footnote{\url{https://huggingface.co/docs}}. The maximum sequence length is set to 128 tokens. All training parameters can be seen in the Appendix \ref{all_parameters}.

A small portion (15\%) of the target dataset is left aside as a \textbf{test set} at the beginning and is used only for the final evaluation to ensure that no test data leak into the training data.

Every fine-tuning, except the one performed on the final test set, is evaluated using a 10-fold \gls{cv}. This helps to get more realistic estimates of model performance than a simple train-validation split would give. The only evaluation metric used for all experiments is the F1 score with \textit{macro} averaging\footnote{The F1 score is computed for both classes and averaged.}. 

All training has been done on the RCI cluster node with 4 x NVIDIA Tesla V100 with 32GB GPU graphic memory.





 \section{Baseline}
 As a baseline, all Czech and multilingual models listed are fine-tuned on BABE and evaluated using a 10-fold stratified \gls{cv}. 
 
 Because of the novelty of the CWNC, I also perform a baseline evaluation over this dataset, but later it is only used as an auxiliary dataset.
 
 The hyperparameters used are the same as those used by the authors of BABE \cite{Spinde2021MBIC}. However, the authors used early stopping together with \gls{cv} and used the validation split inside \gls{cv} to early stop. This may lead to data leakage. Which can subsequently lead to too optimistic results.
 
 A solution to this problem would require another split for validation, but at this point, the size of the training data is already shrunk significantly. Therefore, I did not use early stopping together with \gls{cv} at all. Instead, I fixed the number of epochs to 3 as suggested by the authors of BERT \cite{devlin2019bert} . 
 All other hyperaparemeters remained unchanged; AdamW optimizer is used with an initial learning rate of 5e-5 and a batch size of 64.
 
 The baseline evaluation of all Czech models used can be seen in table \ref{table:3}. The final F1 score is averaged across all folds.
 
 The model that performs best on the BABE is \textbf{FERNET-C5}. It also performs best on the novel CWNC dataset; therefore, it is a suitable candidate for further tuning. From now on, all experiments are performed using this model.
 

 \input{my_modules/tables/baseline}

 
 
 
 
 \section{Hyperparameter tuning}
I restricted the search space of hyperparameters only to the combinations of:
 \begin{itemize}
     \item \textbf{Batch size} $\in \{16,32\}$
     \item \textbf{Learning rate} $\in $ \{2e-5,3e-5,5e-5\}
     \item \textbf{Epochs} $\in \{2,3,4\}$
 \end{itemize}
 
 As the authors of the original BERT paper suggest \cite{devlin2019bert}. Then I ran a grid search with \gls{cv}. The overall best parameters were as follows:
 \begin{center}
      \{learning\_rate = 3e-5,batch\_size = 32, epochs=3\}\label{hyperparams}
 \end{center}
 
 The model with the best parameters achieved a 0.784 F1 score ($\sim$0.4\% improvement against baseline).


 
 





\section{Combining Datasets}
This section is dedicated to the study of the influence of pre-training on combinations of datasets. Trying all combinations would result in training 511 models\footnote{Given a set of $n$ elements, number of subsets is $2^n$. Here, we have a set of nine datasets, resulting in 512 subsets. 511 without an empty set.}, which is infeasible. Therefore, I decided to experiment with pre-training on five subsets of datasets with regard to their bias information, to see which of them can serve as a good initialization for fine-tuning on BABE.
The combination sets are as follows:
\begin{itemize}
    \item \textbf{SUBJe} - is a combination of the SUBJ and MPQA dataset, both of which focus on explicit subjective bias.
    \item \textbf{MB} - is a combination of NJNJ, UA-crisis and BASIL dataset which are all from the \gls{mb} family.
    \item \textbf{WIKI} - are all datasets collected from Wikipedia. It consists of CW-hard, WikiBias, and CWNC. The three datasets were collected automatically with respect to NPOV violations as described in \ref{wiki-npov}.
    \item \textbf{ALL} - This one is simply a combination of all datasets except the WNC.
    \item \textbf{WNC} - WNC is almost 90\% of all data; therefore, I perform experiments on this dataset separately.
\end{itemize}
 
In every combination, the data were randomly mixed and subsequently downsampled, so that the classes were balanced. For each combination, 20\% of data were used as a validation set to decide the optimal number of epochs for pre-training. The convergence of validation losses can be seen in the figure \ref{fig:all_losses}. The number of epochs for pretraining were chosen as follows: 1, 3, 1, 2, 1 for SUBJe, MB, WIKI, ALL and WNC respectively.

This procedure yields five pre-trained models for further experiments.

\begin{figure}
  \includegraphics[scale=0.5]{my_modules/multimedia/all_losses.png}
  \caption{Convergence of validation loss over different dataset combinations}
  \label{fig:all_losses}
\end{figure}

\input{my_modules/tables/all_trained}

\subsection{Pre-training + Evaluating}
To answer the question \hyperref[Q1]{Q1}, the pre-trained models from the previous section are evaluated on BABE without any fine-tuning on it. This way, it can be studied how well each model trained on each set can transfer knowledge to the detection of \gls{mb} in BABE. Thus, possibly unveils the relatedness to BABE. The results can be seen in table \ref{table:4}. In the table, this pre-training \textbf{without} further fine-tuning is referred to as \textbf{Pre-trained}.

Models pre-trained on Wikipedia data, both \textbf{WIKI} and \textbf{WNC} perform relatively well compared to \textbf{MB} and \textbf{SUBJ}. This suggests that the bias distribution in WIKI datasets is the closest to BABE.

The best performing model was pre-trained on the largest dataset, WNC. The model achieved an F1 score of 0.67 on the target dataset which is 21\% more than the model pre-trained on MB set

During pre-training, the F1 score of the WIKI and the MB set both peaked around 70\% on their validation sets; however, the model trained on MB generalized very poorly to BABE data as opposed to the WIKI model (0.46 against 0.63). I suspect that the low quality of the MB set and high size imbalance between the two sets played an important role in this result.

In conclusion, models that generalized to BABE best were pre-trained on Wikipedia datasets.

\subsection{Pre-training + Fine-tuning}
Secondly, pre-trained models are used as a sort of weight initialization for subsequent \textbf{fine-tuning} on the BABE. The results can be seen in table \ref{table:4}. This process is referred to as \textbf{Pre-trained + Fine-tuned}.

Fine-tuning a model pre-trained on \textbf{ALL} datasets combined resulted in the best performance; however, virtually the same performance was achieved by the model trained on the \textbf{SUBJe} combination. Importantly, the SUBJ split represents almost half of all data (see \ref{fig:cz_data}). Therefore, I assume that the performance of ALL model is high because of the presence of SUBJe in the training data.

Using the rest of the pre-trained models for fine-tuning actually hurt the performance. Yet, the difference is very small. The lowest score is achieved by fine-tuning the MB model. I suspect this is mainly due to the small size (2500 sentences) of the balanced MB set that may have led to overfitting.

In conclusion, using models pre-trained on the subjective datasets improved the performance of fine-tuning on BABE, but the increase was very small.




\section{Final evaluation}\label{classifier}
The final FERNET-C5 model has been pre-trained with \textbf{ALL} datasets combination and fine-tuned with the optimal parameters (\ref{hyperparams}) on BABE target dataset. Finally, on a \textbf{test} set it achieved an F1 score of \textbf{0.804}. A confusion matrix of predictions on the test set can be seen below \ref{fig:confmat}

\begin{figure}[h]
  \includegraphics[scale=0.5]{my_modules/multimedia/confmat.png}
  \caption{Confusion matrix of predictions on the test set.}
  \label{fig:confmat}
\end{figure}

For the final model that I share\footnote{\url{https://huggingface.co/horychtom/czech_media_bias_classifier}} on HuggingFace and which is used for inference experiments, the entire BABE dataset, including the test set, was used for training.

\section{Discussion}
The results suggest that subjectivity bias as opposed to media bias appears to be a bit more explicit and straightforward, since pre-training on the subjectivity task helped with \gls{mb} detection, but proved insufficient without further fine-tuning (only 55\% on BABE). This supports the assumption that \gls{mb} is composed of many more superficial linguistic features (\ref{features}).

Also, despite the relatively high performance, the final score on the test set is not representative due to its size. The test set consists of $\sim$ 500 sentences and therefore, may not adequately represent all bias information. For a better evaluation, I propose using a nested \gls{cv} \cite{stone1974cross}.

The authors of the original paper that introduces BABE \cite{spinde2021neural} report an F1 score of $\sim$ 0.8 for fine-tuned transformer model. That is approximately 2\% higher score than I achieved on the Czech version. This may be caused either by the noise introduced into the data during machine translation or by the possibly lower quality of the Czech pre-trained models.

Perhaps, a complete study with more models could be performed, but that would require an enormous number of trained models. Essentially, these results show that there was a minimal gain over the baseline (\textbf{+0.7\%}).

The results indicate that the overall low quality and limited size of the available datasets make their use for media bias detection impractical.






