\chapter{Experiments}\label{experiments}
In this section I present experiments on text classification over colelcted datasets. Main target dataset for evaluations is BABE, because of its high quality and properties.

Because of novelty of CWNC I also perform evaluation on this dataset.
I follow the current standard approaches and use pretrained transformers for further pretraining and fine-tuning. One possibility is to use multilingual models, which are trained on set of languages to capture general language properties. In recent years, there have also purely czech models emerged.
\subsection{Czech models}
\begin{itemize}
    \item RobeCZECH
    \item Czert
    \item FERNET-C5
    \item FERNET-News
\end{itemize}


\subsection{Multi-lingual models}
\begin{itemize}
    \item SlavicBert
    \item mBERT
\end{itemize}


 15\% of BABE data has been saved for final evaluation. Steps are as follows:
 \begin{itemize}
     \item BASELINE:
	5-CV plain model finetuning on BABE (could be split to train validation but we dont have that many data)
	\item HYPERPARAM-TUNING:
	5-CV model tuning parameters (grid search) on BABE
	\item DATA-COMBINATIONS:
	5-CV model with tuned hyperparams with varying combinations of data on BABE
	\item EVALUATION:
	pick the best model, train it with params and early stopping, and run it on - BABE test set 
										       - CWNC test set
	early stopping only on final training! Because the authors essentially made early stopping on "testing set" + it is not recommended to use early stopping in cross validation
 \end{itemize}
 
 
 
 
 
All training has been done on a single GPU on RCI cluster.
 \section{Baseline setup}
 As a baseline, I finetuned all Czech models listed above on BABE and evaluated them using 5-fold stratified cross validation. Hyperparameters were the same as used by authors \cite{Spinde2021MBIC}. However, the authors early stopping together with a cross validation and used the validation split inside CV to early stop, which is not ideal since the split should be used only for evalaution. This way the model can "see" the data before evaluation, hence I did not use early stopping with CV at all and fixed the number of epochs to 3, as authors of BERT \cite{devlin2019bert} suggest. 
 All other hyperaparemeters remained unchanged. AdamW optimizer is used with the initial learning rate is 5e-5. 
 
 Baseline evaluation of all used Czech models can be seen in table \ref{table:3}. Final F1 score is averaged across all folds. For further experiments and tuning, I chose the two best performing models \textbf{RobeCzech} and \textbf{FERNET-C5}.
 
 
 
 
 \section{Hyperparameter tuning}
For choosing the best hyperparameters, I restricted the search space only to the combination of:
 \begin{itemize}
     \item \textbf{Batch size} $\in \{16,32\}$
     \item \textbf{Learning rate} $\in $ \{2e-5,3e-5,5e-5\}
     \item \textbf{Epochs} $\in \{2,3,4\}$
 \end{itemize}
 
 As the authors of the original BERT paper suggest. 
 
  \input{my_modules/tables/baseline}

 
 
 \section{Combining Datasets}
 Here i can combine datasets.
 \subsection{Pretraining on English WNC}
 \subsection{Subjectivity pretraining}
 \subsection{Media Bias Pretraining}
 \subsection{Combining all datasets together Without WNC}

 
 
 
 
\section{Inference on Czech News Samples}
\begin{itemize}
    \item Analysis and statistics
    \item Few words Article level
\end{itemize}

%\section{LIME analysis and demo}

\section{Multi-Task learning approach}\label{mtl}